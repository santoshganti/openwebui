{"path":"wiki/Artificial Intelligence/Coursework/HW1.pdf","text":"Sai Santosh Kumar Ganti U31736172 CS640 Responsible AI RESPONSIBLE AI !1 Q1. Joy argues that bias can be tackled by considering: • Who codes matters - While it is true that programmer to a certain extent determines the bias involved in the system because of the background the individual comes from but by having a team of people of multi-cultural faiths and religions and by having systems reviewed from time to time is the key to eliminating bias in the system. • How we code matters - The algorithms should able to sift through datasets that are biased in nature and let’s suppose the dataset used as training data is biased the AI after that will have ﬂawed results in prediction. So keeping in mind how we handle these cases can be helpful in eliminating bias. So far we have discussed only racial bias but there is far more bias than that happening in the real world, gender bias is one such issue. Another point, I want to bring to notice is the problem of “light illumination” in computer vision. By having more training data in various lighting conditions, this problem can be avoided. • Why we code matters - People have long since tried to minimize or eliminate bias in the systems of criminal justice. And it makes you wonder if it's just difﬁcult for humans to administer alone. In some jurisdictions, basic AI-driven systems help courts assess various risks, from the likelihood that a defendant will skip bail to the likelihood that a potential parolee will re-offend. But the caveat is that humans would still need to guide the process because in most cases these systems affect lives of millions of people and hence policymakers need to adopt more open, transparent and consistent rules for the use of AI in this context if they want to ensure just outcomes. RESPONSIBLE AI !2 Joy argues of an AI system that is used by judges in decision making for the prison sentence with a risk score based system. The algorithm assigns a risk score to each defendant in the court in an effort to predict how likely the defendant will commit a crime in the future. There have been reports all over the internet and news media where the system seems to have racial bias I.e., the black people seem to get a higher risk score than white people. We examine if this is a responsible use of AI. Now there are ways to tackle this problem and it comes back to some of the questions that joy asked in her talk. Who code matters, How we code matters and why we code matters. The people involved in developing the AI in the ﬁrst place come from backgrounds and prejudices of their own and the bias may start from there but this can be solved by having a review system in place where people from different cultures and faiths review the system principles and provide feedback and this way we can have a system that is impartial to racial bias. But the results of the system in its current state should be taken with a grain of salt and human common, for now, should prevail in these sensitive matters. One of the problems of the AI systems using machine learning algorithms to predict if the defendant commits crimes in an engineering context is the fact that ML algorithms remove the outliers and as such some of the minority groups will be ignored and that may include the black people in some areas etc. Also as a society, we are improving as the days go but however, it appears that most of the old values are locked into the internet where much of the training data for machine learning algorithms are derived. Studies have demonstrated that, for example, ‘man’ is associated with a boss, president, leader, and director, whereas ‘woman’ is associated with a helper, assistant, employee, and aide. Google searches for black names like Leroy and Keisha yield ads associated with crime, whereas white names like Brad and Katie yield sites with contact details. In Conclusion, the moral of this tale is simple. We must take a precautionary approach to the use of AI. We must not rely on the possibility of future ﬁxes but instead, make decisions based on what the technology is capable of today. RESPONSIBLE AI !3 References: 1. “The impact of gender and race bias in AI,” The ICRC in New Delhi, 26-Oct-2018. [Online]. Available: https://blogs.icrc.org/law-and-policy/2018/08/28/impact-gender-race- bias-ai/. [Accessed: 11-Feb-2019]. 2. “Responsible AI Practices – Google AI,” Google AI. [Online]. Available: https:// ai.google/education/responsible-ai-practices. [Accessed: 11-Feb-2019]. 3. A. Liptak, “Sent to Prison by a Software Program's Secret Algorithms,” The New York Times, 01-May-2017. [Online]. Available: https://www.nytimes.com/2017/05/01/us/ politics/sent-to-prison-by-a-software-programs-secret-algorithms.html. [Accessed: 11- Feb-2019]. 4. S. Buranyi, “Rise of the racist robots – how AI is learning all our worst impulses,” The Guardian, 08-Aug-2017. [Online]. Available: https://www.theguardian.com/inequality/2017/ aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses. [Accessed: 11- Feb-2019]. 5. H. Sassaman, “Is AI racist? Researchers question algorithms used in law enforcement decisions,” Newsweek, 24-Jan-2018. [Online]. Available: https://www.newsweek.com/ai- racist-yet-computer-algorithms-are-helping-decide-court-cases-789296. [Accessed: 11- Feb-2019]. 6. J. Buolamwini, “How I'm ﬁghting bias in algorithms,” ted. [Online]. Available: https:// www.ted.com/talks/joy_buolamwini_how_i_m_ﬁghting_bias_in_algorithms. [Accessed: 11- Feb-2019]. 7. C. Watney and C. Watney, “It's time for our justice system to embrace artiﬁcial intelligence,” brookings.edu, 09-May-2018. [Online]. Available: https://www.brookings.edu/ blog/techtank/2017/07/20/its-time-for-our-justice-system-to-embrace-artiﬁcial- intelligence/. [Accessed: 11-Feb-2019]. RESPONSIBLE AI !4 Sai Santosh Kumar Ganti U31736172 CS 640 Rule-based Systems RULE BASED SYSTEMS !1 Questions Assume your knowledge base working memory contains the follows observations about an animal splashy Splashy has feathers splashy lays eggs Splashy doesn't fly Splashy is blackandwhite splashy swims a Use Circuit visualization with tame stamps to show howthe zoo KEPER rule based system applies backwards chaining fromthe following hypothesis in oedema Splashy is an ostrich b Splashy is a penguin splashy is an Albatross Take care to use the hypothesis inthe specifiedorder and indicate changes to the working memory when they occur Got Splashy Is an ostrich zim 7 vgsg cannot brogueout CAdding h lsmbhydoa.rsy7 memory splashyhas feathers a splashy is a penguin mIi ax B d IBlackandwhiteT Getsfrom working memory Fatf fwimsI thy Hypothesis Splashy is a penguin Sai Santosh Kumar Ganti U31736172 CS 640 Logic and Planning LOGIC AND PLANNING !1 Convert the following blocks world aisations into anions in it orderlogic Rember to use uppercase letters for constants and lowercase letters for variables Block A is on Black B Block B is on the table BlockC is on the table 9 on blockA Block D is a cube and not a pyramid If block E is on the table then it is not on BlockD All cubes are green There enests a block 2 such that tf x is a not a pyramid then 2 is on foe call x candy it is true that it blockx is a pyramid then blocky is not on n solution Function Orkney such that nisony Equal my such that nis equaltoy Cube n such that n is a cube pyramidG such that nts a pyramid Goreencn such that n is green On AaB On B Table On Citable V 0nA AS Cube D n Pyramid D Once Table ONCED fn Cube n GreenCND Pyramidon 3z on G in Fn y pyramids 7 Only D which of the above assertions are disjunctions solution is a disjunction on citable on c A which of the above alsations contain free variable solution Contains a free variable n Convert the anions in the previous engageconto clause form solution are literals is a clause Cube D n Pyramid D ubc D T Pyramid D betoken into 2 literals On E table onceD using a B A VB oncetable V AVB An Cube n Gaeenin then CubeCn V Green n Pyramid n Fzfonttin using function i above n Pyramid a on aboveCnbn Pyramid n v onCabovecn n Pyramidon on above n forty Pyramidon onlynD Anty f Pyramid n V on lying Pyramid n V oncn.gg Desume we add the situation antom Black C is not on Block A to the list above what new enpression can weinfer what is the name of the inference rule you used soy on CC A On Citable von Cit using modus posers we have Resolvent On Citable provide a resolution proof that shows that black D is green For each step in your proof namethe inference rule you are using and dearly indicate which cantons you are combining to derive new rules and wba the resolvent is solution we have to prove the following GreenLD we solve it using green's trick GreenLDV Answeg Green a v Answer Cube n Green n Instantiate Cn D using modus Ponens 7 CubeCD Answer Cubecog using modus ponens Answer Selfdriving cool Isolation we have to proove the following 35g Infront D C St Negation of the above theorem Fg Infront D C a Fg T InfrontLD CSD Infront D 4 D we add use the following cantons Behind DIGS Open D s Anfyfs Behind mitis nopenths Infront nis Maxcy Antyfs Behind mitis nopenCM D VInfrontGifMovelas 181 V nfytz.fi Behind miff V Openthis InfrontGifMoudyDD Behindfr y s v openCms VInfrontGisMoudyD Now we dothe resolution proof using Green's trick Infront D CSf Answer D4SD 7 Behindth Y D V 7 open mis VInfront lay movelyis n D y C g MoveCis Behind D GD V TopenCBDvAnswer DGs Behind DLD s s f using modus posers 7 Open Dis Answer DC SD Open Dis using modusPonens i Sai Santosh Kumar Ganti U31736172 CS 640 ROC Analysis ROC ANALYSIS !1 Roc Analysis Solution Confusion Makin True SameBabyNotsameBabyI Same 200 Tp 40 Fp predicted NotSame 10 150 TN ITotalImages 4oosol20 True positive CTB 200 True Negative TN _150 False positive FP 40 false Negative CAD ro Sol Accuracy TPtTN 350 Too 875 Sd false positive Rate FIE 49 0 21051 Sd 200 True positive Rate TP z o 95 LY THEN Sd Sensitivity TPR 95 24 Solid specificity INp I FPR 78.954 Sd Fi Suge 2 1 2 Precision Recall 2 f Recealt TPR qg.zy.JAI.tno 2 2o qg 8q O 88 Sol we need to improve the FPR of system because children who are identified as false Negative will get to take the drug again On the other hand people who are identified as False positive will not getthe drug again which is really bad perfosmance from our system Sol A bigger area underthe curve is like dealing cenoth Percentages the y may look appealing butthe actualvalues of the dataset may not be that high Similarly a bigger area under the curve means that for various cutoff Values System it is more accurate than systemB This doesn't translate the usefulness of system A onessystems system A could have higherfalse positive crate wheocho the actual metric of concern","libVersion":"0.5.0","langs":""}